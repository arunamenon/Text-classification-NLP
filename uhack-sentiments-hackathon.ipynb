{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Data description","metadata":{"papermill":{"duration":0.024935,"end_time":"2021-12-27T11:31:45.520504","exception":false,"start_time":"2021-12-27T11:31:45.495569","status":"completed"},"tags":[],"id":"06bc110b"}},{"cell_type":"markdown","source":"<img style=\"-webkit-user-select: none;margin: auto;cursor: zoom-out;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;\" src=\"https://machinehack-be.s3.amazonaws.com/uhack_sentiments_20_decode_code_words/Ugam_large%281%29.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAI2O7AQTB6JBT4VSA%2F20211231%2Fap-south-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20211231T060318Z&amp;X-Amz-Expires=172800&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=8122ce25af5594b76da91526229193ca14266f4ff27824f63e19399decd45b77\" width=\"1024\" height=\"168\">","metadata":{"papermill":{"duration":0.025286,"end_time":"2021-12-27T11:31:45.569922","exception":false,"start_time":"2021-12-27T11:31:45.544636","status":"completed"},"tags":[],"id":"ddb3bd9b"}},{"cell_type":"markdown","source":"The challenge here is to analyze and deep dive into the natural language text (reviews) and bucket them based on their topics of discussion. Furthermore, analyzing the overall sentiment will also help the business to make tangible decisions.\n\nThe data set provided to you has a mix of customer reviews for products across categories and retailers. We would like you to model on the data to bucket the future reviews in their respective topics (Note: A review can talk about multiple topics)\n\nOverall polarity (positive/negative sentiment)\n\n \n\nTrain: 6136 rows x 14 columns\nTest: 2631 rows x 14 columns \n \n\nTopics (Components, Delivery and Customer Support, Design and Aesthetics, Dimensions, Features, Functionality, Installation, Material, Price, Quality and Usability)\nPolarity (Positive/Negative)\nNote: The target variables are all encoded in the train dataset for convenience. Please submit the test results in the similar encoded fashion for us to evaluate your results.","metadata":{"papermill":{"duration":0.023498,"end_time":"2021-12-27T11:31:45.619828","exception":false,"start_time":"2021-12-27T11:31:45.59633","status":"completed"},"tags":[],"id":"033b46b6"}},{"cell_type":"markdown","source":"## Import packages","metadata":{"papermill":{"duration":0.023146,"end_time":"2021-12-27T11:31:45.666271","exception":false,"start_time":"2021-12-27T11:31:45.643125","status":"completed"},"tags":[],"id":"78f34efb"}},{"cell_type":"code","source":"!pip install clean-text --user\n!pip install optuna --user\n!pip install boostaroota --user\n!pip install transformers --user\n!pip install sentencepiece --user\n!pip install wordninja\n!pip install autocorrect\n!pip install language_tool_python\n!pip install pyspellchecker","metadata":{"id":"r5_sDVcxUnC1","outputId":"99ff333c-e458-4efa-834f-ae55032c7068","execution":{"iopub.status.busy":"2021-12-31T14:40:53.312343Z","iopub.execute_input":"2021-12-31T14:40:53.313481Z","iopub.status.idle":"2021-12-31T14:41:58.907975Z","shell.execute_reply.started":"2021-12-31T14:40:53.313402Z","shell.execute_reply":"2021-12-31T14:41:58.907257Z"},"trusted":true},"execution_count":230,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom tqdm import tqdm\nimport nltk\nimport warnings \nwarnings.filterwarnings(\"ignore\")\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# from flashtext import KeywordProcessor\nimport re\nfrom nltk.tokenize import sent_tokenize\nimport gensim \nfrom gensim.models import Word2Vec\nfrom gensim import models\n# from fuzzywuzzy import fuzz\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom nltk.corpus import stopwords\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nimport matplotlib.pyplot as plt\nimport matplotlib_venn as venn\nfrom matplotlib_venn import venn3, venn3_circles, venn3_unweighted\nfrom tqdm import tqdm\n# from unidecode import unidecode\nfrom sklearn.decomposition import PCA   \nimport re\n\n# from imblearn.over_sampling import RandomOverSampler\n# from imblearn.over_sampling import ADASYN\n\n# from biobert_embedding.embedding import BiobertEmbedding\nfrom scipy.spatial import distance\nimport pandas as pd\n\nfrom nltk import ngrams\nfrom cleantext import clean\nimport plotly.graph_objects as go\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import confusion_matrix, recall_score, precision_score, precision_recall_curve, auc, f1_score, \\\n    average_precision_score, accuracy_score, roc_curve\n\nfrom xgboost.sklearn import XGBClassifier\nfrom optuna.samplers import TPESampler\nimport functools\nimport xgboost as xgb\nimport sklearn\n\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import mean_squared_error,roc_auc_score,precision_score\nfrom sklearn import metrics\nimport optuna\nfrom boostaroota import BoostARoota\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.multioutput import ClassifierChain\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nimport pickle\nimport joblib\n\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import model_selection as sk_model_selection\nfrom xgboost import plot_tree\nimport shap\n\nimport h2o\nfrom h2o.automl import H2OAutoML\nimport language_tool_python\nimport wordninja\nfrom autocorrect import Speller\nfrom spellchecker import SpellChecker\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nSEED=42","metadata":{"papermill":{"duration":1.629494,"end_time":"2021-12-27T11:31:47.322145","exception":false,"start_time":"2021-12-27T11:31:45.692651","status":"completed"},"tags":[],"id":"640d1f34","outputId":"a10c3a05-cf5b-47ff-fcab-e52dc29ed1c4","execution":{"iopub.status.busy":"2021-12-31T14:41:58.910465Z","iopub.execute_input":"2021-12-31T14:41:58.910751Z","iopub.status.idle":"2021-12-31T14:41:58.931683Z","shell.execute_reply.started":"2021-12-31T14:41:58.910721Z","shell.execute_reply":"2021-12-31T14:41:58.930756Z"},"trusted":true},"execution_count":231,"outputs":[]},{"cell_type":"code","source":"import torch\nimport io\nimport torch.nn.functional as F\nimport random\nimport numpy as np\nimport time\nimport math\nimport datetime\nimport torch.nn as nn\nfrom transformers import *\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed_val)","metadata":{"id":"cNCu9hGkWV5o","execution":{"iopub.status.busy":"2021-12-31T14:41:58.932832Z","iopub.execute_input":"2021-12-31T14:41:58.933450Z","iopub.status.idle":"2021-12-31T14:41:58.949296Z","shell.execute_reply.started":"2021-12-31T14:41:58.933404Z","shell.execute_reply":"2021-12-31T14:41:58.948156Z"},"trusted":true},"execution_count":232,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{"papermill":{"duration":0.025487,"end_time":"2021-12-27T11:31:47.371879","exception":false,"start_time":"2021-12-27T11:31:47.346392","status":"completed"},"tags":[],"id":"c8b36726"}},{"cell_type":"code","source":"filepath = '../input/uhack-sentiments-20-decode-code-words'\ntrain_data = pd.read_csv(filepath + r'/train.csv')\ntest_data = pd.read_csv(filepath + r'/test.csv')\nsubmission_data = pd.read_csv(filepath + r'/submission.csv')","metadata":{"papermill":{"duration":0.133695,"end_time":"2021-12-27T11:31:47.529039","exception":false,"start_time":"2021-12-27T11:31:47.395344","status":"completed"},"tags":[],"id":"bc599438","execution":{"iopub.status.busy":"2021-12-31T14:41:58.951277Z","iopub.execute_input":"2021-12-31T14:41:58.951681Z","iopub.status.idle":"2021-12-31T14:41:59.017156Z","shell.execute_reply.started":"2021-12-31T14:41:58.951641Z","shell.execute_reply":"2021-12-31T14:41:59.016102Z"},"trusted":true},"execution_count":233,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{"papermill":{"duration":0.023574,"end_time":"2021-12-27T11:31:47.576856","exception":false,"start_time":"2021-12-27T11:31:47.553282","status":"completed"},"tags":[],"id":"3dbbd4a1"}},{"cell_type":"code","source":"print(train_data.shape)\ntrain_data.head()","metadata":{"papermill":{"duration":0.053984,"end_time":"2021-12-27T11:31:47.654818","exception":false,"start_time":"2021-12-27T11:31:47.600834","status":"completed"},"tags":[],"id":"6869932a","outputId":"e36d5b71-b5bd-4e6c-f42e-ae7a4555f92a","execution":{"iopub.status.busy":"2021-12-31T14:41:59.018750Z","iopub.execute_input":"2021-12-31T14:41:59.018966Z","iopub.status.idle":"2021-12-31T14:41:59.036514Z","shell.execute_reply.started":"2021-12-31T14:41:59.018941Z","shell.execute_reply":"2021-12-31T14:41:59.035509Z"},"trusted":true},"execution_count":234,"outputs":[]},{"cell_type":"code","source":"print(test_data.shape)\ntest_data.head()","metadata":{"papermill":{"duration":0.049075,"end_time":"2021-12-27T11:31:47.729482","exception":false,"start_time":"2021-12-27T11:31:47.680407","status":"completed"},"tags":[],"id":"ecda1041","outputId":"90343028-fe29-4b43-e85e-6feb8d01e85c","execution":{"iopub.status.busy":"2021-12-31T14:41:59.038041Z","iopub.execute_input":"2021-12-31T14:41:59.038292Z","iopub.status.idle":"2021-12-31T14:41:59.061070Z","shell.execute_reply.started":"2021-12-31T14:41:59.038260Z","shell.execute_reply":"2021-12-31T14:41:59.060023Z"},"trusted":true},"execution_count":235,"outputs":[]},{"cell_type":"code","source":"print(submission_data.shape)\nsubmission_data.head()","metadata":{"papermill":{"duration":0.048992,"end_time":"2021-12-27T11:31:47.804216","exception":false,"start_time":"2021-12-27T11:31:47.755224","status":"completed"},"tags":[],"id":"e112321c","outputId":"285355e2-e827-4909-d656-8fafa933bf41","execution":{"iopub.status.busy":"2021-12-31T14:41:59.062451Z","iopub.execute_input":"2021-12-31T14:41:59.063395Z","iopub.status.idle":"2021-12-31T14:41:59.077260Z","shell.execute_reply.started":"2021-12-31T14:41:59.063354Z","shell.execute_reply":"2021-12-31T14:41:59.076184Z"},"trusted":true},"execution_count":236,"outputs":[]},{"cell_type":"code","source":"categories = [x for x in train_data.columns.tolist() if x not in ['Id','Review','Polarity']]","metadata":{"papermill":{"duration":0.036018,"end_time":"2021-12-27T11:31:47.86807","exception":false,"start_time":"2021-12-27T11:31:47.832052","status":"completed"},"tags":[],"id":"3ae9c929","execution":{"iopub.status.busy":"2021-12-31T14:41:59.078456Z","iopub.execute_input":"2021-12-31T14:41:59.079108Z","iopub.status.idle":"2021-12-31T14:41:59.088681Z","shell.execute_reply.started":"2021-12-31T14:41:59.079071Z","shell.execute_reply":"2021-12-31T14:41:59.087994Z"},"trusted":true},"execution_count":237,"outputs":[]},{"cell_type":"markdown","source":"### Distribution by length","metadata":{"papermill":{"duration":0.026461,"end_time":"2021-12-27T11:31:47.921364","exception":false,"start_time":"2021-12-27T11:31:47.894903","status":"completed"},"tags":[],"id":"ef8b3b04"}},{"cell_type":"code","source":"def distr_by_length(category):\n    data = train_data.copy()\n    if category!='All':\n        data = data[data[category]==1].reset_index(drop = True)\n    print('\\n# Reviews:', data.shape[0])\n    print('Average polarity:', data['Polarity'].mean() * 100,'%\\n')\n    temp = data['Review'].apply(lambda x: len(x.split())).reset_index()\n    display(temp[['Review']].describe().reset_index())\n    fig = px.histogram(temp, x=\"Review\")\n    fig.show()\n    \nw = widgets.interactive(distr_by_length, category = ['All'] + categories)\ndisplay(w)","metadata":{"papermill":{"duration":1.218355,"end_time":"2021-12-27T11:31:49.167261","exception":false,"start_time":"2021-12-27T11:31:47.948906","status":"completed"},"tags":[],"id":"14c2f0b0","outputId":"7efe9c56-5b8b-4366-b5da-b3db4ab3048b","execution":{"iopub.status.busy":"2021-12-31T14:41:59.090168Z","iopub.execute_input":"2021-12-31T14:41:59.090962Z","iopub.status.idle":"2021-12-31T14:41:59.201814Z","shell.execute_reply.started":"2021-12-31T14:41:59.090921Z","shell.execute_reply":"2021-12-31T14:41:59.200777Z"},"trusted":true},"execution_count":238,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of categories and polarity","metadata":{"papermill":{"duration":0.026933,"end_time":"2021-12-27T11:31:49.2277","exception":false,"start_time":"2021-12-27T11:31:49.200767","status":"completed"},"tags":[],"id":"6a4293e5"}},{"cell_type":"code","source":"train_data[categories].sum(axis = 1).min(), train_data[categories].sum(axis = 1).max()","metadata":{"papermill":{"duration":0.041002,"end_time":"2021-12-27T11:31:49.29579","exception":false,"start_time":"2021-12-27T11:31:49.254788","status":"completed"},"tags":[],"id":"1da4bfd2","outputId":"5c424c0a-3e56-4a3d-8eeb-24e546903f63","execution":{"iopub.status.busy":"2021-12-31T14:41:59.205878Z","iopub.execute_input":"2021-12-31T14:41:59.206408Z","iopub.status.idle":"2021-12-31T14:41:59.217904Z","shell.execute_reply.started":"2021-12-31T14:41:59.206347Z","shell.execute_reply":"2021-12-31T14:41:59.216876Z"},"trusted":true},"execution_count":239,"outputs":[]},{"cell_type":"code","source":"train_data['Polarity'].describe().reset_index()","metadata":{"papermill":{"duration":0.043942,"end_time":"2021-12-27T11:31:49.366925","exception":false,"start_time":"2021-12-27T11:31:49.322983","status":"completed"},"tags":[],"id":"1bbd3d3c","outputId":"d6cf18a2-9ef2-4ee4-ab05-94a2c84425a9","execution":{"iopub.status.busy":"2021-12-31T14:41:59.219227Z","iopub.execute_input":"2021-12-31T14:41:59.219633Z","iopub.status.idle":"2021-12-31T14:41:59.236144Z","shell.execute_reply.started":"2021-12-31T14:41:59.219566Z","shell.execute_reply":"2021-12-31T14:41:59.235024Z"},"trusted":true},"execution_count":240,"outputs":[]},{"cell_type":"code","source":"overlap_df = pd.DataFrame(data = None, columns = ['Category'] + categories + ['# Reviews', '% of all reviews'])\nperc_overlap_df = pd.DataFrame(data = None, columns = ['Category'] + categories)\nfor col in categories:\n    n = train_data.groupby([col]).size().reset_index().sort_values(by = col).iloc[-1][0]\n    temp = train_data[train_data[col]==1][categories].sum(axis = 0).reset_index().T\n    temp = temp.iloc[1:,:]\n    temp.columns = categories\n    temp['Category'] = col\n    overlap_df = pd.concat([overlap_df, temp], axis = 0).reset_index(drop = True)\n    overlap_df['# Reviews'].iloc[overlap_df.shape[0]-1] = n\n    overlap_df['% of all reviews'].iloc[overlap_df.shape[0]-1] = n/train_data.shape[0] * 100\n    for x in temp.columns:\n        if x!= 'Category':\n            temp[x] = temp[x]/n * 100\n    perc_overlap_df = pd.concat([perc_overlap_df, temp], axis = 0).reset_index(drop = True)","metadata":{"papermill":{"duration":0.183533,"end_time":"2021-12-27T11:31:49.578362","exception":false,"start_time":"2021-12-27T11:31:49.394829","status":"completed"},"tags":[],"id":"f9c85da0","execution":{"iopub.status.busy":"2021-12-31T14:41:59.239048Z","iopub.execute_input":"2021-12-31T14:41:59.239395Z","iopub.status.idle":"2021-12-31T14:41:59.393127Z","shell.execute_reply.started":"2021-12-31T14:41:59.239345Z","shell.execute_reply":"2021-12-31T14:41:59.392172Z"},"trusted":true},"execution_count":241,"outputs":[]},{"cell_type":"code","source":"overlap_df","metadata":{"papermill":{"duration":0.050174,"end_time":"2021-12-27T11:31:49.655573","exception":false,"start_time":"2021-12-27T11:31:49.605399","status":"completed"},"tags":[],"id":"05fd1afd","outputId":"781fa93c-2643-4e9d-95c3-504f52bae876","execution":{"iopub.status.busy":"2021-12-31T14:41:59.394852Z","iopub.execute_input":"2021-12-31T14:41:59.395166Z","iopub.status.idle":"2021-12-31T14:41:59.416070Z","shell.execute_reply.started":"2021-12-31T14:41:59.395123Z","shell.execute_reply":"2021-12-31T14:41:59.415114Z"},"trusted":true},"execution_count":242,"outputs":[]},{"cell_type":"code","source":"perc_overlap_df[categories].style.applymap(lambda x: 'background-color : yellow' if x>=20 and x!=100 else '')","metadata":{"papermill":{"duration":0.12027,"end_time":"2021-12-27T11:31:49.80404","exception":false,"start_time":"2021-12-27T11:31:49.68377","status":"completed"},"tags":[],"id":"385ea330","outputId":"ec076af5-10de-4e55-949b-df960710e36e","execution":{"iopub.status.busy":"2021-12-31T14:41:59.417777Z","iopub.execute_input":"2021-12-31T14:41:59.418115Z","iopub.status.idle":"2021-12-31T14:41:59.446681Z","shell.execute_reply.started":"2021-12-31T14:41:59.418071Z","shell.execute_reply":"2021-12-31T14:41:59.445821Z"},"trusted":true},"execution_count":243,"outputs":[]},{"cell_type":"markdown","source":"#### Clean text","metadata":{"id":"3lQgLjQYXl6X"}},{"cell_type":"code","source":"def correct_spelling(x):\n    #remove all punctuations before finding possible misspelled words\n    s = re.sub(r'[^\\w\\s]','',x)\n#     print(\"Text without punctuations:\\n\",s)\n    wordlist=s.split()\n    spell = SpellChecker()\n    # find those words that may be misspelled\n    misspelled = list(spell.unknown(wordlist))\n\n    return ' '.join([spell.correction(i) if i in misspelled else i for i in x.split(' ')])\n\ndef fix_fullstops(x):\n    matches = re.findall('(\\.[a-zA-Z]+)', x)\n    for i in range(0, len(matches)):\n        x = x.replace(matches[i], '. ' + matches[i][1:])\n\n    matches = re.findall('[a-zA-Z]\\s+\\.', x)\n    for i in range(0, len(matches)):\n        x = x.replace(matches[i], matches[i].replace(' ',''))\n        \n    return x\n\ndef fix_combined_words(x):\n    matches = re.findall('[a-z][a-z][A-Z]',x)\n\n    for i in matches:\n        x = x.replace(i, i[0:2] + \" \" + i[2])\n\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:41:59.448095Z","iopub.execute_input":"2021-12-31T14:41:59.448347Z","iopub.status.idle":"2021-12-31T14:41:59.457912Z","shell.execute_reply.started":"2021-12-31T14:41:59.448317Z","shell.execute_reply":"2021-12-31T14:41:59.456906Z"},"trusted":true},"execution_count":244,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\n\n# Remove reduntant spaces\ntrain_data['Review v1'] = train_data['Review'].progress_apply(lambda x: re.sub(' +', ' ',x))\ntest_data['Review v1'] = test_data['Review'].progress_apply(lambda x: re.sub(' +', ' ',x))\n\n# Remove multiple fullstops\ntrain_data['Review v1'] = train_data['Review v1'].progress_apply(lambda x: re.sub(r'\\.+', \".\", x))\ntest_data['Review v1'] = test_data['Review v1'].progress_apply(lambda x: re.sub(r'\\.+', \".\", x))\n\n# Correct spelling\ntrain_data['Review v1'] = train_data['Review v1'].apply(lambda x: correct_spelling(x))\ntest_data['Review v1'] = test_data['Review v1'].apply(lambda x: correct_spelling(x))\n\n# Fix fullstops\ntrain_data['Review v1'] = train_data['Review v1'].apply(lambda x: fix_fullstops(x))\ntest_data['Review v1'] = test_data['Review v1'].apply(lambda x: fix_fullstops(x))\n\n# Break combined words\ntrain_data['Review v1'] = train_data['Review v1'].progress_apply(lambda x: fix_combined_words(x))\ntest_data['Review v1'] = test_data['Review v1'].progress_apply(lambda x: fix_combined_words(x))\n\n# Break combined words\n# train_data['Review v1'] = train_data['Review v1'].progress_apply(lambda x: ' '.join(wordninja.split(x)))\n# test_data['Review v1'] = test_data['Review v1'].progress_apply(lambda x: ' '.join(wordninja.split(x)))\n\n# Correct speling mistakes and grammar\n# tool = language_tool_python.LanguageTool('en-US')\n# train_data['Review v1'] = train_data['Review v1'].progress_apply(lambda x: tool.correct(x))\n# test_data['Review v1'] = test_data['Review v1'].progress_apply(lambda x: tool.correct(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T14:41:59.459497Z","iopub.execute_input":"2021-12-31T14:41:59.459876Z","iopub.status.idle":"2021-12-31T15:00:44.048555Z","shell.execute_reply.started":"2021-12-31T14:41:59.459834Z","shell.execute_reply":"2021-12-31T15:00:44.047640Z"},"trusted":true},"execution_count":245,"outputs":[]},{"cell_type":"code","source":"# tool = language_tool_python.LanguageTool('en-US')\n# tool.correct(train_data['Review'].iloc[6134])","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:00:44.050109Z","iopub.execute_input":"2021-12-31T15:00:44.050360Z","iopub.status.idle":"2021-12-31T15:00:44.054795Z","shell.execute_reply.started":"2021-12-31T15:00:44.050331Z","shell.execute_reply":"2021-12-31T15:00:44.053811Z"},"trusted":true},"execution_count":246,"outputs":[]},{"cell_type":"code","source":"idx = 6134\nprint('\\n')\ndisplay(train_data['Review'].iloc[idx])\nprint('\\n')\ndisplay(train_data['Review v1'].iloc[idx])","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:00:44.056144Z","iopub.execute_input":"2021-12-31T15:00:44.056821Z","iopub.status.idle":"2021-12-31T15:00:44.076352Z","shell.execute_reply.started":"2021-12-31T15:00:44.056787Z","shell.execute_reply":"2021-12-31T15:00:44.075776Z"},"trusted":true},"execution_count":247,"outputs":[]},{"cell_type":"code","source":"idx = 12\nprint('\\n')\ndisplay(train_data['Review'].iloc[idx])\nprint('\\n')\ndisplay(train_data['Review v1'].iloc[idx])","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:00:44.077523Z","iopub.execute_input":"2021-12-31T15:00:44.077790Z","iopub.status.idle":"2021-12-31T15:00:44.091682Z","shell.execute_reply.started":"2021-12-31T15:00:44.077759Z","shell.execute_reply":"2021-12-31T15:00:44.090817Z"},"trusted":true},"execution_count":248,"outputs":[]},{"cell_type":"code","source":"train_data['Review cleaned for tf-idf']=train_data['Review v1'].apply(lambda x : clean(x,\n                                                                                    fix_unicode=True,               # fix various unicode errors\n                                                                                    to_ascii=True,                  # transliterate to closest ASCII representation\n                                                                                    lower=True,                     # lowercase text\n                                                                                    no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n                                                                                    no_urls=True,                  # replace all URLs with a special token\n                                                                                    no_emails=True,                # replace all email addresses with a special token\n                                                                                    no_phone_numbers=True,         # replace all phone numbers with a special token\n                                                                                    no_numbers=True,               # replace all numbers with a special token\n                                                                                    no_digits=True,                # replace all digits with a special token\n                                                                                    no_currency_symbols=True,      # replace all currency symbols with a special token\n                                                                                    no_punct=True,                 # remove punctuations\n                                                                        ))\n\ntest_data['Review cleaned for tf-idf']=test_data['Review v1'].apply(lambda x : clean(x,\n                                                                                    fix_unicode=True,               # fix various unicode errors\n                                                                                    to_ascii=True,                  # transliterate to closest ASCII representation\n                                                                                    lower=True,                     # lowercase text\n                                                                                    no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n                                                                                    no_urls=True,                  # replace all URLs with a special token\n                                                                                    no_emails=True,                # replace all email addresses with a special token\n                                                                                    no_phone_numbers=True,         # replace all phone numbers with a special token\n                                                                                    no_numbers=True,               # replace all numbers with a special token\n                                                                                    no_digits=True,                # replace all digits with a special token\n                                                                                    no_currency_symbols=True,      # replace all currency symbols with a special token\n                                                                                    no_punct=True,                 # remove punctuations\n                                                                        ))\n\n\ntrain_data['Review cleaned for transformers']=train_data['Review v1'].apply(lambda x : clean(x,\n                                                                          lower=False,\n                                                                          no_line_breaks=True,\n                                                                          no_urls=True,\n                                                                          no_emails=True, \n                                                                          no_phone_numbers=True,\n#                                                                           no_punct=True\n                                                                        ))\n\ntest_data['Review cleaned for transformers']=test_data['Review v1'].apply(lambda x : clean(x,\n                                                                          lower=False,\n                                                                          no_line_breaks=True,\n                                                                          no_urls=True,\n                                                                          no_emails=True, \n                                                                          no_phone_numbers=True,\n#                                                                           no_punct=True\n                                                                        ))","metadata":{"id":"r8tSABxQXh4I","execution":{"iopub.status.busy":"2021-12-31T15:00:44.093641Z","iopub.execute_input":"2021-12-31T15:00:44.093965Z","iopub.status.idle":"2021-12-31T15:01:05.968615Z","shell.execute_reply.started":"2021-12-31T15:00:44.093922Z","shell.execute_reply":"2021-12-31T15:01:05.967823Z"},"trusted":true},"execution_count":249,"outputs":[]},{"cell_type":"code","source":"# # Stemming\n# ps = PorterStemmer()\n# train_data['Review cleaned for tf-idf'] = train_data['Review cleaned for tf-idf'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split(' ')]))\n# test_data['Review cleaned for tf-idf'] = test_data['Review cleaned for tf-idf'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split(' ')]))\n\n# Lemmitize\nlmtzr = WordNetLemmatizer()\ntrain_data['Review cleaned for tf-idf'] = train_data['Review cleaned for tf-idf'].apply(lambda x: ' '.join([lmtzr.lemmatize(word) for word in x.split(' ')]))\ntest_data['Review cleaned for tf-idf'] = test_data['Review cleaned for tf-idf'].apply(lambda x: ' '.join([lmtzr.lemmatize(word) for word in x.split(' ')]))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:01:05.969883Z","iopub.execute_input":"2021-12-31T15:01:05.970107Z","iopub.status.idle":"2021-12-31T15:01:06.932744Z","shell.execute_reply.started":"2021-12-31T15:01:05.970080Z","shell.execute_reply":"2021-12-31T15:01:06.931875Z"},"trusted":true},"execution_count":250,"outputs":[]},{"cell_type":"code","source":"idx = 9\nprint('\\n')\ndisplay(train_data['Review'].iloc[idx])\nprint('\\n')\ndisplay(train_data['Review cleaned for tf-idf'].iloc[idx])","metadata":{"id":"l1co3K4oXiCd","outputId":"6d3099a1-69c1-44db-a15d-03dbf539a938","execution":{"iopub.status.busy":"2021-12-31T15:01:06.934326Z","iopub.execute_input":"2021-12-31T15:01:06.934604Z","iopub.status.idle":"2021-12-31T15:01:06.944337Z","shell.execute_reply.started":"2021-12-31T15:01:06.934554Z","shell.execute_reply":"2021-12-31T15:01:06.943482Z"},"trusted":true},"execution_count":251,"outputs":[]},{"cell_type":"markdown","source":"### Modelling","metadata":{"papermill":{"duration":0.030061,"end_time":"2021-12-27T11:31:49.863491","exception":false,"start_time":"2021-12-27T11:31:49.83343","status":"completed"},"tags":[],"id":"a8c9ef70"}},{"cell_type":"code","source":"category = 'Quality'","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:01:06.945662Z","iopub.execute_input":"2021-12-31T15:01:06.945928Z","iopub.status.idle":"2021-12-31T15:01:06.955062Z","shell.execute_reply.started":"2021-12-31T15:01:06.945899Z","shell.execute_reply":"2021-12-31T15:01:06.954336Z"},"trusted":true},"execution_count":252,"outputs":[]},{"cell_type":"code","source":"def classification_metrics(y_true, y_prob):\n    '''\n     Calculates classification metrics\n    :param y_true: true label\n    :param y_prob: probabilitites of true label\n    :param thrshold: threshold\n    :return: metrics\n    '''\n    \n    # calculating auroc values\n    fpr_rf, tpr_rf,thresholds = roc_curve(y_true, y_prob)\n    roc_auc_rf = auc(fpr_rf, tpr_rf)\n    optimal_idx = np.argmax(tpr_rf - fpr_rf)\n    optimal_threshold = thresholds[optimal_idx]\n        \n#         print(optimal_threshold)\n#         print(\"=====\"*20) \n#     if optimal_cal==False:\n#         optimal_threshold = thrshold   \n     \n    # generating prediction on the basis of certain threshold\n    y_pred = np.where(y_prob >= optimal_threshold, 1, 0)\n\n    # calculating tp,tn,fp,fn from confusion metrics\n    tn, fp, fn, tp = (confusion_matrix(y_true, y_pred)).ravel()\n\n    # calculating auprc\n    average_precision = average_precision_score(y_true, y_prob)\n\n    # calculating precision,recall and f1 sscore and accuracy\n    precision = (precision_score(y_true, y_pred))\n    recall = (recall_score(y_true, y_pred))\n    accuracy = (accuracy_score(y_true, y_pred))\n    f1_accuracy = (f1_score(y_true, y_pred))\n    from sklearn.metrics import cohen_kappa_score\n    kappa_score = cohen_kappa_score(y_true, y_pred, labels=None, weights=None)\n    binary_cross_entropy=log_loss(y_true, y_prob)\n    # creating dictionary of classification metric\n    target_mean=np.mean(y_true)\n    classification_metric_dict = {\"True_negatives\": tn,\n                                  \"False_positives\": fp,\n                                  \"False_negatives\": fn,\n                                  \"True_positives\": tp,\n                                  \"Accuracy\": accuracy,\n                                  \"Recall\": recall,\n                                  \"Precision\": precision,\n                                  \"f1_score\": f1_accuracy,\n                                  \"PR_AUC\": average_precision,\n                                  \"ROC_AUC\": roc_auc_rf,\n                                  \"Kappa Score\": kappa_score,\n                                  \"binary_cross_entropy\":binary_cross_entropy,\n                                  \"target_imbalance\":target_mean,\n                                  \"target_size\":len(y_true),\n                                  'optimal_threshold':optimal_threshold\n                                  }\n\n    return classification_metric_dict, optimal_threshold\n\ndef classification_metrics_train(y_true, y_prob,threshold):\n    '''\n     Calculates classification metrics\n    :param y_true: true label\n    :param y_prob: probabilitites of true label\n    :param thrshold: threshold\n    :return: metrics\n    '''\n    \n    # calculating auroc values\n    fpr_rf, tpr_rf,thresholds = roc_curve(y_true, y_prob)\n    roc_auc_rf = auc(fpr_rf, tpr_rf)\n#     optimal_idx = np.argmax(tpr_rf - fpr_rf)\n    optimal_threshold = threshold\n        \n#         print(optimal_threshold)\n#         print(\"=====\"*20) \n#     if optimal_cal==False:\n#         optimal_threshold = thrshold   \n     \n    # generating prediction on the basis of certain threshold\n    y_pred = np.where(y_prob >= optimal_threshold, 1, 0)\n\n    # calculating tp,tn,fp,fn from confusion metrics\n    tn, fp, fn, tp = (confusion_matrix(y_true, y_pred)).ravel()\n\n    # calculating auprc\n    average_precision = average_precision_score(y_true, y_prob)\n\n    # calculating precision,recall and f1 sscore and accuracy\n    precision = (precision_score(y_true, y_pred))\n    recall = (recall_score(y_true, y_pred))\n    accuracy = (accuracy_score(y_true, y_pred))\n    f1_accuracy = (f1_score(y_true, y_pred))\n    from sklearn.metrics import cohen_kappa_score\n    kappa_score = cohen_kappa_score(y_true, y_pred, labels=None, weights=None)\n    binary_cross_entropy=log_loss(y_true, y_prob)\n    # creating dictionary of classification metric\n    target_mean=np.mean(y_true)\n    classification_metric_dict = {\"True_negatives\": tn,\n                                  \"False_positives\": fp,\n                                  \"False_negatives\": fn,\n                                  \"True_positives\": tp,\n                                  \"Accuracy\": accuracy,\n                                  \"Recall\": recall,\n                                  \"Precision\": precision,\n                                  \"f1_score\": f1_accuracy,\n                                  \"PR_AUC\": average_precision,\n                                  \"ROC_AUC\": roc_auc_rf,\n                                  \"Kappa Score\": kappa_score,\n                                  \"binary_cross_entropy\":binary_cross_entropy,\n                                  \"target_imbalance\":target_mean,\n                                  \"target_size\":len(y_true),\n                                  'optimal_thresh':optimal_threshold\n                                  }\n\n    return classification_metric_dict, optimal_threshold","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:01:06.957293Z","iopub.execute_input":"2021-12-31T15:01:06.957532Z","iopub.status.idle":"2021-12-31T15:01:06.973888Z","shell.execute_reply.started":"2021-12-31T15:01:06.957503Z","shell.execute_reply":"2021-12-31T15:01:06.973154Z"},"trusted":true},"execution_count":253,"outputs":[]},{"cell_type":"code","source":"def generate_bert_embeddings(data,column_name,hf_model):\n    sentences=data[column_name].tolist()\n\n    tokenizer = AutoTokenizer.from_pretrained(hf_model)\n    model = AutoModel.from_pretrained(hf_model)\n\n    #Tokenize sentences\n    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors='pt')\n\n    #Compute token embeddings\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n        sentence_embeddings = model_output[0][:,0] #Take the first token ([CLS]) from each sentence \n\n\n    df=pd.DataFrame(sentence_embeddings)\n    cols = [\"bert_vector\"+\"_\"+str(i) for i in np.arange(0,df.shape[1],1)]\n    df.columns=cols\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:01:06.975211Z","iopub.execute_input":"2021-12-31T15:01:06.975731Z","iopub.status.idle":"2021-12-31T15:01:06.993379Z","shell.execute_reply.started":"2021-12-31T15:01:06.975696Z","shell.execute_reply":"2021-12-31T15:01:06.992458Z"},"trusted":true},"execution_count":254,"outputs":[]},{"cell_type":"code","source":"sample_data_with_embeddings = generate_bert_embeddings(train_data.head(10),'Review cleaned for transformers','bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:01:06.994679Z","iopub.execute_input":"2021-12-31T15:01:06.995373Z","iopub.status.idle":"2021-12-31T15:01:21.498560Z","shell.execute_reply.started":"2021-12-31T15:01:06.995340Z","shell.execute_reply":"2021-12-31T15:01:21.497485Z"},"trusted":true},"execution_count":255,"outputs":[]},{"cell_type":"code","source":"sample_data_with_embeddings.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:01:21.500385Z","iopub.execute_input":"2021-12-31T15:01:21.500655Z","iopub.status.idle":"2021-12-31T15:01:21.564377Z","shell.execute_reply.started":"2021-12-31T15:01:21.500621Z","shell.execute_reply":"2021-12-31T15:01:21.563510Z"},"trusted":true},"execution_count":256,"outputs":[]},{"cell_type":"code","source":"# generate_bert_embeddings(train_data,'Review cleaned for transformers','emilyalsentzer/bert-base-uncased')\nvectorizer = TfidfVectorizer(stop_words='english',ngram_range=(1, 1))\n\ntrain_df, valid_df  = sk_model_selection.train_test_split(\n    train_data, \n    test_size=0.2, \n    random_state=SEED,\n    stratify = train_data[category])\n\nX_train = train_df['Review cleaned for tf-idf']\ny_train = train_df[category]\nX_valid = valid_df['Review cleaned for tf-idf']\ny_valid = valid_df[category]\n\nX_train = pd.DataFrame(vectorizer.fit_transform(X_train).todense())\nX_valid = pd.DataFrame(vectorizer.transform(X_valid).todense())","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:01:21.568857Z","iopub.execute_input":"2021-12-31T15:01:21.569122Z","iopub.status.idle":"2021-12-31T15:01:21.791764Z","shell.execute_reply.started":"2021-12-31T15:01:21.569090Z","shell.execute_reply":"2021-12-31T15:01:21.790973Z"},"trusted":true},"execution_count":257,"outputs":[]},{"cell_type":"code","source":"# PCA - Alternatives include TSNE, UMAP, Autoencoder\npca = PCA(2000,random_state=SEED)  \nprojected_df = pca.fit_transform(X_train)\n\nexp_var_pca = pca.explained_variance_ratio_\n#\n# Cumulative sum of eigenvalues; This will be used to create step plot\n# for visualizing the variance explained by each principal component.\n#\ncum_sum_eigenvalues = np.cumsum(exp_var_pca)\n#\n# Create the visualization plot\n#\nplt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\nplt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal component index')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:01:21.792803Z","iopub.execute_input":"2021-12-31T15:01:21.793455Z","iopub.status.idle":"2021-12-31T15:02:01.030705Z","shell.execute_reply.started":"2021-12-31T15:01:21.793421Z","shell.execute_reply":"2021-12-31T15:02:01.029878Z"},"trusted":true},"execution_count":258,"outputs":[]},{"cell_type":"code","source":"X_train = pd.DataFrame(pca.transform(X_train))\nX_train.columns = ['PCA - ' + str(x) for x in X_train.columns]\ndisplay(X_train.head())\n\nX_valid = pd.DataFrame(pca.transform(X_valid))\nX_valid.columns = ['PCA - ' + str(x) for x in X_valid.columns]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:02:01.032266Z","iopub.execute_input":"2021-12-31T15:02:01.032765Z","iopub.status.idle":"2021-12-31T15:02:03.334598Z","shell.execute_reply.started":"2021-12-31T15:02:01.032722Z","shell.execute_reply":"2021-12-31T15:02:03.333552Z"},"trusted":true},"execution_count":259,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, X_valid.shape)\ny_train.sum()/len(y_train) * 100, y_valid.sum()/len(y_valid) * 100 ","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:02:03.336434Z","iopub.execute_input":"2021-12-31T15:02:03.337084Z","iopub.status.idle":"2021-12-31T15:02:03.348211Z","shell.execute_reply.started":"2021-12-31T15:02:03.337024Z","shell.execute_reply":"2021-12-31T15:02:03.347294Z"},"trusted":true},"execution_count":260,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic regression","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(max_iter = 1000, solver = 'lbfgs', random_state = SEED, class_weight = 'balanced' )\nparameters = {'C':[0.001, 0.01, 0.1, 1, 10, 100]}\nclf_lr = GridSearchCV(lr, parameters, cv = 5).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:02:03.350054Z","iopub.execute_input":"2021-12-31T15:02:03.350662Z","iopub.status.idle":"2021-12-31T15:02:18.826108Z","shell.execute_reply.started":"2021-12-31T15:02:03.350615Z","shell.execute_reply":"2021-12-31T15:02:18.825089Z"},"trusted":true},"execution_count":261,"outputs":[]},{"cell_type":"code","source":"clf_lr","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:02:18.831447Z","iopub.execute_input":"2021-12-31T15:02:18.834521Z","iopub.status.idle":"2021-12-31T15:02:18.850291Z","shell.execute_reply.started":"2021-12-31T15:02:18.834433Z","shell.execute_reply":"2021-12-31T15:02:18.849314Z"},"trusted":true},"execution_count":262,"outputs":[]},{"cell_type":"code","source":"clf_lr.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:02:18.852378Z","iopub.execute_input":"2021-12-31T15:02:18.853110Z","iopub.status.idle":"2021-12-31T15:02:18.860404Z","shell.execute_reply.started":"2021-12-31T15:02:18.853062Z","shell.execute_reply":"2021-12-31T15:02:18.859571Z"},"trusted":true},"execution_count":263,"outputs":[]},{"cell_type":"code","source":"y_preds_lr = clf_lr.best_estimator_.predict_proba(X_valid)[:,1]\nreport_valid, t = classification_metrics(y_valid, y_preds_lr)\nprint('\\nValidation data log loss:', log_loss(y_valid, y_preds_lr))\nprint('Optimal threhsold:',t)\ndisplay(pd.DataFrame(report_valid,[category]))\n\ny_preds_lr = clf_lr.best_estimator_.predict_proba(X_train)[:,1]\nreport_train,_ = classification_metrics_train(y_train, y_preds_lr, t)\nprint('\\nTrain data log loss:', log_loss(y_train, y_preds_lr))\ndisplay(pd.DataFrame(report_train,[category]))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:02:18.862190Z","iopub.execute_input":"2021-12-31T15:02:18.862767Z","iopub.status.idle":"2021-12-31T15:02:19.060948Z","shell.execute_reply.started":"2021-12-31T15:02:18.862723Z","shell.execute_reply":"2021-12-31T15:02:19.060040Z"},"trusted":true},"execution_count":264,"outputs":[]},{"cell_type":"code","source":"# ROC Curve: Area Under the Curve\ndef auc_roc_plot(y_test, y_preds):\n    fpr, tpr, thresholds = roc_curve(y_test,y_preds)\n    roc_auc = auc(fpr, tpr)\n    print(roc_auc)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:02:19.062741Z","iopub.execute_input":"2021-12-31T15:02:19.063341Z","iopub.status.idle":"2021-12-31T15:02:19.072497Z","shell.execute_reply.started":"2021-12-31T15:02:19.063292Z","shell.execute_reply":"2021-12-31T15:02:19.071538Z"},"trusted":true},"execution_count":265,"outputs":[]},{"cell_type":"code","source":"y_preds_lr = clf_lr.best_estimator_.predict_proba(X_valid)[:,1]\nauc_roc_plot(y_valid, y_preds_lr)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:02:19.074383Z","iopub.execute_input":"2021-12-31T15:02:19.074983Z","iopub.status.idle":"2021-12-31T15:02:19.322376Z","shell.execute_reply.started":"2021-12-31T15:02:19.074937Z","shell.execute_reply":"2021-12-31T15:02:19.321379Z"},"trusted":true},"execution_count":266,"outputs":[]},{"cell_type":"code","source":"X_test = pd.DataFrame(vectorizer.transform(test_data['Review cleaned for tf-idf']).todense())\nX_test = pd.DataFrame(pca.transform(X_test))\nX_test.columns = ['PCA - ' + str(x) for x in X_test.columns]\n\nsubmission_data = test_data[['Id']]\nsubmission_data[category] = clf_lr.best_estimator_.predict_proba(X_test)[:,1]\nsubmission_data.to_csv(category + '_submission_lr.csv', index=False)\nprint(submission_data.shape)\ndisplay(submission_data.head())","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:02:19.324223Z","iopub.execute_input":"2021-12-31T15:02:19.324568Z","iopub.status.idle":"2021-12-31T15:02:20.429551Z","shell.execute_reply.started":"2021-12-31T15:02:19.324513Z","shell.execute_reply":"2021-12-31T15:02:20.428648Z"},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"markdown","source":"#### SVM","metadata":{}},{"cell_type":"code","source":"svc = SVC(random_state=SEED, class_weight='balanced',probability=True, verbose=True, max_iter = 100)\nparameters = {'C':[0.1, 1, 10]}\n\nprint(X_train.shape, y_train.sum(), y_train.sum()/X_train.shape[0])\n\noversample = ''\nif oversample=='ADASYN':\n    oversampler = ADASYN(sampling_strategy=0.5,random_state=SEED,n_neighbors=5)\n    X_train,y_train=oversampler.fit_resample(X_train, y_train)\n\n    print(X_train.shape, y_train.sum(), y_train.sum()/X_train.shape[0])\n\nclf_svc = GridSearchCV(svc, parameters, cv = 2).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:02:20.430905Z","iopub.execute_input":"2021-12-31T15:02:20.431498Z","iopub.status.idle":"2021-12-31T15:03:48.910110Z","shell.execute_reply.started":"2021-12-31T15:02:20.431452Z","shell.execute_reply":"2021-12-31T15:03:48.909010Z"},"trusted":true},"execution_count":268,"outputs":[]},{"cell_type":"code","source":"y_preds_svc = clf_svc.best_estimator_.predict_proba(X_valid)[:,1]\nreport_valid, t = classification_metrics(y_valid, y_preds_svc)\nprint('Validation data log loss:', log_loss(y_valid, y_preds_svc))\nprint('Optimal threhsold:',t)\ndisplay(pd.DataFrame(report_valid,['Loan Status']))\n\ny_preds_svc = clf_svc.best_estimator_.predict_proba(X_train)[:,1]\nreport_train,_ = classification_metrics_train(y_train, y_preds_svc, t)\nprint('Train data log loss:', log_loss(y_train, y_preds_svc))\ndisplay(pd.DataFrame(report_train,['Loan Status']))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:03:48.911265Z","iopub.execute_input":"2021-12-31T15:03:48.911805Z","iopub.status.idle":"2021-12-31T15:03:52.498258Z","shell.execute_reply.started":"2021-12-31T15:03:48.911757Z","shell.execute_reply":"2021-12-31T15:03:52.497405Z"},"trusted":true},"execution_count":269,"outputs":[]},{"cell_type":"code","source":"y_preds_svc = clf_svc.best_estimator_.predict_proba(X_valid)[:,1]\nauc_roc_plot(y_valid, y_preds_svc)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:03:52.499396Z","iopub.execute_input":"2021-12-31T15:03:52.499657Z","iopub.status.idle":"2021-12-31T15:03:53.396437Z","shell.execute_reply.started":"2021-12-31T15:03:52.499624Z","shell.execute_reply":"2021-12-31T15:03:53.395701Z"},"trusted":true},"execution_count":270,"outputs":[]},{"cell_type":"code","source":"X_test = pd.DataFrame(vectorizer.transform(test_data['Review cleaned for tf-idf']).todense())\nX_test = pd.DataFrame(pca.transform(X_test))\nX_test.columns = ['PCA - ' + str(x) for x in X_test.columns]\n\nsubmission_data = test_data[['Id']]\nsubmission_data[category] = clf_svc.best_estimator_.predict_proba(X_test)[:,1]\nsubmission_data.to_csv(category + '_submission_svc.csv', index=False)\nprint(submission_data.shape)\ndisplay(submission_data.head())","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:03:53.397861Z","iopub.execute_input":"2021-12-31T15:03:53.398419Z","iopub.status.idle":"2021-12-31T15:03:56.010396Z","shell.execute_reply.started":"2021-12-31T15:03:53.398384Z","shell.execute_reply":"2021-12-31T15:03:56.009514Z"},"trusted":true},"execution_count":271,"outputs":[]},{"cell_type":"markdown","source":"#### XGBoost","metadata":{}},{"cell_type":"code","source":"def objective_classification(X_train, y_train, X_val, y_val, target_value, trial):\n    \"\"\"It tries to find the best hyper-parameters for XGBOOST model for given task\n\n        Details:\n            It uses OPTUNA library which is based on Baseian-optimization to tune the hyper-params.\n\n        Args:\n            X_train: training data\n            X_test: testing data\n            y_tain: training label\n            y_val: validation label\n            trial: object of optuna for optimizing the task in hand\n\n        Returns:\n            best score till now\n\n    \"\"\"\n    if ((target_value)):\n        tree_methods = ['approx', 'hist', 'exact']\n        boosting_lists = ['gbtree', 'gblinear']\n        objective_list_reg = ['binary:logistic']  # 'reg:gamma', 'reg:tweedie'\n        boosting = trial.suggest_categorical('boosting', boosting_lists),\n        tree_method = trial.suggest_categorical('tree_method', tree_methods),\n        n_estimator = trial.suggest_int('n_estimators',20,200, 10),\n        max_depth = trial.suggest_int('max_depth',2,16),\n        reg_alpha = trial.suggest_int('reg_alpha',2,5),\n        reg_lambda = trial.suggest_int('reg_lambda',2,5),\n        min_child_weight = trial.suggest_int('min_child_weight', 1,5),\n        gamma = trial.suggest_int('gamma',2,5),\n        learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.5),\n        objective = trial.suggest_categorical('objective', objective_list_reg),\n        colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree', 0.5, 1, 0.05),\n        colsample_bynode = trial.suggest_discrete_uniform('colsample_bynode', 0.5, 1, 0.05),\n        colsample_bylevel = trial.suggest_discrete_uniform('colsample_bylevel', 0.5, 1, 0.05),\n        subsample = trial.suggest_discrete_uniform('subsample', 0.8, 1, 0.1),\n#         scale_pos_weight = len(y_train)/sum(y_train)\n#         scale_pos_weight = trial.suggest_discrete_uniform('scale_pos_weight',10,100, 10)\n        scale_pos_weight = trial.suggest_discrete_uniform('scale_pos_weight',1,5, 0.1)\n        nthread = -1\n        \n          \n    xgboost_tune = xgb.XGBClassifier(\n        # tree_method=tree_method[0],\n        boosting=boosting[0],\n        reg_alpha=reg_alpha[0],\n        reg_lambda=reg_lambda[0],\n        gamma=gamma[0],\n        objective=objective[0],\n        colsample_bynode=colsample_bynode[0],\n        colsample_bylevel=colsample_bylevel[0],\n        n_estimators=n_estimator[0],\n        max_depth=max_depth[0],\n        min_child_weight=min_child_weight[0],\n        learning_rate=learning_rate[0],\n        subsample=subsample[0],\n        colsample_bytree=colsample_bytree[0],\n        scale_pos_weight=scale_pos_weight,\n        n_jobs=nthread,\n        random_state=SEED)\n    \n    xgboost_tune.fit(X_train, y_train)\n    pred_val = xgboost_tune.predict_proba(X_val)[:, 1]\n\n#     pred_val[pred_val < 0] = 0\n#     pred_val = np.nan_to_num(pred_val)\n\n    fpr_rf, tpr_rf, _ = roc_curve(y_val, pred_val)\n    roc_auc_rf = auc(fpr_rf, tpr_rf)\n    \n    #average_precision = average_precision_score(y_val,pred_val)\n#     a,b=(classification_metrics(y_val,pred_val))\n    \n    #precision, recall, thresholds = precision_recall_curve(y_val,pred_val)\n    #area_under_curve = auc(recall, precision)\n\n    return roc_auc_rf\n#     pred_val = xgboost_tune.predict(X_val)\n#     return sklearn.metrics.precision_score(y_val,pred_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:30:02.647712Z","iopub.execute_input":"2021-12-31T15:30:02.648795Z","iopub.status.idle":"2021-12-31T15:30:02.661000Z","shell.execute_reply.started":"2021-12-31T15:30:02.648748Z","shell.execute_reply":"2021-12-31T15:30:02.660247Z"},"trusted":true},"execution_count":302,"outputs":[]},{"cell_type":"code","source":"train_data[train_data['Id']==928]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:30:02.662699Z","iopub.execute_input":"2021-12-31T15:30:02.663037Z","iopub.status.idle":"2021-12-31T15:30:02.690051Z","shell.execute_reply.started":"2021-12-31T15:30:02.663008Z","shell.execute_reply":"2021-12-31T15:30:02.689221Z"},"trusted":true},"execution_count":303,"outputs":[]},{"cell_type":"code","source":"# generate_bert_embeddings(train_data,'Review cleaned for transformers','emilyalsentzer/bert-base-uncased')\nvectorizer = TfidfVectorizer(stop_words='english',ngram_range=(1, 1))\n\ntrain_df, valid_df  = sk_model_selection.train_test_split(\n    train_data, \n    test_size=0.2, \n    random_state=SEED,\n    stratify = train_data[category])\n\nX_train = train_df['Review cleaned for tf-idf'].reset_index(drop = True)\ny_train = train_df[category].reset_index(drop = True)\nX_valid = valid_df['Review cleaned for tf-idf'].reset_index(drop = True)\ny_valid = valid_df[category].reset_index(drop = True)\n\nX_train = pd.DataFrame(vectorizer.fit_transform(X_train).todense())\nX_valid = pd.DataFrame(vectorizer.transform(X_valid).todense())\n\ninv_map = {v: k for k, v in vectorizer.vocabulary_.items()}\nX_train.columns = ['Feature - ' + inv_map[x] for x in X_train.columns]\nX_valid.columns = ['Feature - ' + inv_map[x] for x in X_valid.columns]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:30:02.691532Z","iopub.execute_input":"2021-12-31T15:30:02.692124Z","iopub.status.idle":"2021-12-31T15:30:03.139162Z","shell.execute_reply.started":"2021-12-31T15:30:02.692087Z","shell.execute_reply":"2021-12-31T15:30:03.138146Z"},"trusted":true},"execution_count":304,"outputs":[]},{"cell_type":"code","source":"print('Total # features for modelling before boostaroota:', X_train.shape[1])\n\nbr = BoostARoota(metric='logloss', silent = True)\nbr.fit(X_train,y_train)\nX_train=X_train[br.keep_vars_.tolist()]\nX_valid=X_valid[br.keep_vars_.tolist()]\nprint('Total # features for modelling after boostaroota:', len(br.keep_vars_.tolist()))\n\nstudy = optuna.create_study(direction='maximize', sampler=TPESampler(seed=SEED))\nstudy.optimize(\n    functools.partial(objective_classification, X_train, y_train, X_valid, y_valid,'Binary'),\n            timeout=500)\n\nmodel_xgb = xgb.XGBClassifier(**study.best_params, random_state=SEED)\nmodel_xgb.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:30:03.141210Z","iopub.execute_input":"2021-12-31T15:30:03.141459Z","iopub.status.idle":"2021-12-31T15:41:12.186388Z","shell.execute_reply.started":"2021-12-31T15:30:03.141426Z","shell.execute_reply":"2021-12-31T15:41:12.185308Z"},"trusted":true},"execution_count":305,"outputs":[]},{"cell_type":"code","source":"y_predicted = model_xgb.predict_proba(X_valid)[:,1]\nreport_valid, t = classification_metrics(y_valid, y_predicted)\nprint('Validation data log loss:', log_loss(y_valid, y_predicted))\nprint('Optimal threhsold:',t)\ndisplay(pd.DataFrame(report_valid,[category]))\n\ny_predicted = model_xgb.predict_proba(X_train)[:,1]\nreport_train, _ = classification_metrics_train(y_train, y_predicted, t)\nprint('Train data log loss:', log_loss(y_train, y_predicted))\npd.DataFrame(report_train,[category])","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:12.188187Z","iopub.execute_input":"2021-12-31T15:41:12.188521Z","iopub.status.idle":"2021-12-31T15:41:12.299803Z","shell.execute_reply.started":"2021-12-31T15:41:12.188476Z","shell.execute_reply":"2021-12-31T15:41:12.298861Z"},"trusted":true},"execution_count":306,"outputs":[]},{"cell_type":"code","source":"print(\"Saving model .. \",end=\" \")\njoblib.dump(model_xgb,\"XGBoost_model.pkl\")","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:12.301036Z","iopub.execute_input":"2021-12-31T15:41:12.301312Z","iopub.status.idle":"2021-12-31T15:41:12.351035Z","shell.execute_reply.started":"2021-12-31T15:41:12.301278Z","shell.execute_reply":"2021-12-31T15:41:12.350443Z"},"trusted":true},"execution_count":307,"outputs":[]},{"cell_type":"code","source":"y_predicted = model_xgb.predict_proba(pd.concat([X_train, X_valid], axis = 0))[:,1]\nprint('Entire data log loss:', log_loss(pd.concat([y_train, y_valid], axis = 0), y_predicted))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:12.352402Z","iopub.execute_input":"2021-12-31T15:41:12.352889Z","iopub.status.idle":"2021-12-31T15:41:12.393772Z","shell.execute_reply.started":"2021-12-31T15:41:12.352855Z","shell.execute_reply":"2021-12-31T15:41:12.392767Z"},"trusted":true},"execution_count":308,"outputs":[]},{"cell_type":"code","source":"model_xgb.n_estimators, model_xgb.max_depth","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:12.395065Z","iopub.execute_input":"2021-12-31T15:41:12.395774Z","iopub.status.idle":"2021-12-31T15:41:12.402972Z","shell.execute_reply.started":"2021-12-31T15:41:12.395726Z","shell.execute_reply":"2021-12-31T15:41:12.401924Z"},"trusted":true},"execution_count":309,"outputs":[]},{"cell_type":"code","source":"explainer = shap.TreeExplainer(model_xgb)\nshap_values = explainer.shap_values(X_train.head(100))\nshap.initjs()\nplt.clf()\nshap.summary_plot(shap_values,features=X_train.head(100))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:12.404369Z","iopub.execute_input":"2021-12-31T15:41:12.404616Z","iopub.status.idle":"2021-12-31T15:41:13.411719Z","shell.execute_reply.started":"2021-12-31T15:41:12.404560Z","shell.execute_reply":"2021-12-31T15:41:13.410561Z"},"trusted":true},"execution_count":310,"outputs":[]},{"cell_type":"code","source":"explainer.expected_value","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:13.414538Z","iopub.execute_input":"2021-12-31T15:41:13.414826Z","iopub.status.idle":"2021-12-31T15:41:13.420701Z","shell.execute_reply.started":"2021-12-31T15:41:13.414791Z","shell.execute_reply":"2021-12-31T15:41:13.419765Z"},"trusted":true},"execution_count":311,"outputs":[]},{"cell_type":"code","source":"np.mean(model_xgb.predict(X_train))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:13.421978Z","iopub.execute_input":"2021-12-31T15:41:13.422187Z","iopub.status.idle":"2021-12-31T15:41:13.462265Z","shell.execute_reply.started":"2021-12-31T15:41:13.422161Z","shell.execute_reply":"2021-12-31T15:41:13.461313Z"},"trusted":true},"execution_count":312,"outputs":[]},{"cell_type":"code","source":"class ShapObject:\n    \n    def __init__(self, base_values, data, values, feature_names):\n        self.base_values = base_values # Single value\n        self.data = data # Raw feature values for 1 row of data\n        self.values = values # SHAP values for the same row of data\n        self.feature_names = feature_names # Column names\n        \n\nshap_object = ShapObject(base_values = explainer.expected_value,\n                         values = shap_values[0,:],\n                         feature_names = X_train.columns,\n                         data = X_train.iloc[0,:])\n\nshap.waterfall_plot(shap_object)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:13.463462Z","iopub.execute_input":"2021-12-31T15:41:13.463958Z","iopub.status.idle":"2021-12-31T15:41:13.976074Z","shell.execute_reply.started":"2021-12-31T15:41:13.463920Z","shell.execute_reply":"2021-12-31T15:41:13.975200Z"},"trusted":true},"execution_count":313,"outputs":[]},{"cell_type":"code","source":"shap.plots.force(explainer.expected_value,shap_values[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:13.977719Z","iopub.execute_input":"2021-12-31T15:41:13.977978Z","iopub.status.idle":"2021-12-31T15:41:13.987354Z","shell.execute_reply.started":"2021-12-31T15:41:13.977945Z","shell.execute_reply":"2021-12-31T15:41:13.986550Z"},"trusted":true},"execution_count":314,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(dpi=180)\nax = plt.subplot(1,1,1)\n\nplot_tree(model_xgb, num_trees=0, ax = ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:13.988849Z","iopub.execute_input":"2021-12-31T15:41:13.989516Z","iopub.status.idle":"2021-12-31T15:41:17.059724Z","shell.execute_reply.started":"2021-12-31T15:41:13.989469Z","shell.execute_reply":"2021-12-31T15:41:17.058568Z"},"trusted":true},"execution_count":315,"outputs":[]},{"cell_type":"code","source":"temp = pd.DataFrame(model_xgb.feature_importances_)\ntemp['Feature'] = X_train.columns.tolist()\ntemp.columns = ['Feature importance','Feature']\ntemp[temp['Feature importance']>0][['Feature importance','Feature']].sort_values(by = 'Feature importance', ascending = False).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:17.061761Z","iopub.execute_input":"2021-12-31T15:41:17.062772Z","iopub.status.idle":"2021-12-31T15:41:17.085503Z","shell.execute_reply.started":"2021-12-31T15:41:17.062722Z","shell.execute_reply":"2021-12-31T15:41:17.084882Z"},"trusted":true},"execution_count":316,"outputs":[]},{"cell_type":"code","source":"model_xgb_full_data = xgb.XGBClassifier(**study.best_params, random_state=SEED)\nmodel_xgb_full_data.fit(pd.concat([X_train,X_valid], axis = 0), pd.concat([y_train,y_valid], axis = 0))\n\ny_predicted = model_xgb_full_data.predict_proba(pd.concat([X_train,X_valid], axis = 0))[:,1]\nprint('Entire data log loss:', log_loss(pd.concat([y_train, y_valid], axis = 0), y_predicted))\n\nprint(\"Saving model .. \",end=\" \")\njoblib.dump(model_xgb_full_data,\"XGBoost_model_full_data.pkl\")","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:17.086564Z","iopub.execute_input":"2021-12-31T15:41:17.087170Z","iopub.status.idle":"2021-12-31T15:41:30.392196Z","shell.execute_reply.started":"2021-12-31T15:41:17.087133Z","shell.execute_reply":"2021-12-31T15:41:30.391216Z"},"trusted":true},"execution_count":317,"outputs":[]},{"cell_type":"code","source":"X_test = pd.DataFrame(vectorizer.transform(test_data['Review cleaned for tf-idf']).todense())\nX_test.columns = ['Feature - ' + inv_map[x] for x in X_test.columns]\n\nxgboost_pred = model_xgb.predict_proba(X_test[X_train.columns.tolist()])[:,1]\n# xgboost_pred = [1 if x>=t else 0 for x in xgboost_pred]\n\nsubmission_data = test_data[['Id']]\nsubmission_data[category] = xgboost_pred\nsubmission_data.to_csv(category + '_submission_xgboost.csv', index=False)\nprint(submission_data.shape)\ndisplay(submission_data.head())\n\nxgboost_pred = model_xgb_full_data.predict_proba(X_test[X_train.columns.tolist()])[:,1]\n# xgboost_pred = [1 if x>=t else 0 for x in xgboost_pred]\n\nsubmission_data = test_data[['Id']]\nsubmission_data[category] = xgboost_pred\nsubmission_data.to_csv(category + '_submission_xgboost_full_data_train.csv', index=False)\nprint(submission_data.shape)\ndisplay(submission_data.head())","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:41:30.394020Z","iopub.execute_input":"2021-12-31T15:41:30.394579Z","iopub.status.idle":"2021-12-31T15:41:30.648065Z","shell.execute_reply.started":"2021-12-31T15:41:30.394536Z","shell.execute_reply":"2021-12-31T15:41:30.647145Z"},"trusted":true},"execution_count":318,"outputs":[]},{"cell_type":"markdown","source":"#### H20.ai","metadata":{}},{"cell_type":"code","source":"# generate_bert_embeddings(train_data,'Review cleaned for transformers','emilyalsentzer/bert-base-uncased')\nvectorizer = TfidfVectorizer(stop_words='english',ngram_range=(1, 1))\n\ntrain_df, valid_df  = sk_model_selection.train_test_split(\n    train_data, \n    test_size=0.2, \n    random_state=SEED,\n    stratify = train_data[category])\n\nX_train = train_df['Review cleaned for tf-idf'].reset_index(drop = True)\ny_train = train_df[[category]].reset_index(drop = True)\nX_valid = valid_df['Review cleaned for tf-idf'].reset_index(drop = True)\ny_valid = valid_df[[category]].reset_index(drop = True)\n\nX_train = pd.DataFrame(vectorizer.fit_transform(X_train).todense())\nX_valid = pd.DataFrame(vectorizer.transform(X_valid).todense())\n\ninv_map = {v: k for k, v in vectorizer.vocabulary_.items()}\nX_train.columns = ['Feature - ' + inv_map[x] for x in X_train.columns]\nX_valid.columns = ['Feature - ' + inv_map[x] for x in X_valid.columns]\n\n# PCA - Alternatives include TSNE, UMAP, Autoencoder\npca = PCA(500,random_state=SEED)  \nprojected_df = pca.fit_transform(X_train)\n\nexp_var_pca = pca.explained_variance_ratio_\n#\n# Cumulative sum of eigenvalues; This will be used to create step plot\n# for visualizing the variance explained by each principal component.\n#\ncum_sum_eigenvalues = np.cumsum(exp_var_pca)\n#\n# Create the visualization plot\n#\nplt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\nplt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal component index')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()\n\nX_train = pd.DataFrame(pca.transform(X_train))\nX_train.columns = ['PCA - ' + str(x) for x in X_train.columns]\ndisplay(X_train.head())\n\nX_valid = pd.DataFrame(pca.transform(X_valid))\nX_valid.columns = ['PCA - ' + str(x) for x in X_valid.columns]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:58:31.694406Z","iopub.execute_input":"2021-12-31T15:58:31.694726Z","iopub.status.idle":"2021-12-31T15:58:41.598603Z","shell.execute_reply.started":"2021-12-31T15:58:31.694683Z","shell.execute_reply":"2021-12-31T15:58:41.597473Z"},"trusted":true},"execution_count":323,"outputs":[]},{"cell_type":"code","source":"h2o.init(\n    nthreads=-1,     # number of threads when launching a new H2O server\n    max_mem_size=12  # in gigabytes\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:58:41.605578Z","iopub.execute_input":"2021-12-31T15:58:41.608822Z","iopub.status.idle":"2021-12-31T15:58:41.676489Z","shell.execute_reply.started":"2021-12-31T15:58:41.608743Z","shell.execute_reply":"2021-12-31T15:58:41.675639Z"},"trusted":true},"execution_count":324,"outputs":[]},{"cell_type":"code","source":"def _convert_h2oframe_to_numeric(h2o_frame, training_columns):\n    for column in training_columns:\n        h2o_frame[column] = h2o_frame[column].asnumeric()\n    return h2o_frame","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:58:41.678075Z","iopub.execute_input":"2021-12-31T15:58:41.683455Z","iopub.status.idle":"2021-12-31T15:58:41.692543Z","shell.execute_reply.started":"2021-12-31T15:58:41.683377Z","shell.execute_reply":"2021-12-31T15:58:41.691348Z"},"trusted":true},"execution_count":325,"outputs":[]},{"cell_type":"code","source":"X_y_train_h = h2o.H2OFrame(\n    pd.concat(\n        [pd.concat([X_train, X_valid], axis = 0), pd.concat([y_train, y_valid], axis = 0)],\n        axis='columns'\n    )\n)\n\nfeature_cols = X_train.columns.tolist()\n\nX_y_train_h = _convert_h2oframe_to_numeric(X_y_train_h, feature_cols)\nX_y_train_h[category] = X_y_train_h[category].asfactor()\n\naml = H2OAutoML(\n    max_runtime_secs=(int(3600 * 0.25)),  # hours\n    max_models=None,  # no limit\n    balance_classes=True,\n    seed=SEED)\n\naml.train(\n    x=feature_cols,\n    y=category,\n    training_frame=X_y_train_h)\n\nlb = aml.leaderboard\nmodel_ids = list(lb['model_id'].as_data_frame().iloc[:,0])\nout_path = \".\"\n\nfor m_id in model_ids:\n    mdl = h2o.get_model(m_id)\n    h2o.save_model(model=mdl, path=out_path, force=True)\n\nh2o.export_file(lb, os.path.join(out_path, 'aml_leaderboard.h2o'), force=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:58:41.695112Z","iopub.execute_input":"2021-12-31T15:58:41.695788Z","iopub.status.idle":"2021-12-31T16:16:12.399018Z","shell.execute_reply.started":"2021-12-31T15:58:41.695726Z","shell.execute_reply":"2021-12-31T16:16:12.398070Z"},"trusted":true},"execution_count":326,"outputs":[]},{"cell_type":"code","source":"models_path = \".\"\nlb = h2o.import_file(path=os.path.join(models_path, \"aml_leaderboard.h2o\"))\nlb.head(rows=20)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:16:12.403579Z","iopub.execute_input":"2021-12-31T16:16:12.403981Z","iopub.status.idle":"2021-12-31T16:16:12.710387Z","shell.execute_reply.started":"2021-12-31T16:16:12.403929Z","shell.execute_reply":"2021-12-31T16:16:12.709615Z"},"trusted":true},"execution_count":327,"outputs":[]},{"cell_type":"code","source":"X_test = test_data['Review cleaned for tf-idf'].reset_index(drop = True)\nX_test = pd.DataFrame(vectorizer.transform(X_test).todense())\nX_test.columns = ['Feature - ' + inv_map[x] for x in X_test.columns]\nX_test = pd.DataFrame(pca.transform(X_test))\nX_test.columns = ['PCA - ' + str(x) for x in X_test.columns]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:16:12.712046Z","iopub.execute_input":"2021-12-31T16:16:12.712604Z","iopub.status.idle":"2021-12-31T16:16:13.282815Z","shell.execute_reply.started":"2021-12-31T16:16:12.712546Z","shell.execute_reply":"2021-12-31T16:16:13.281860Z"},"trusted":true},"execution_count":328,"outputs":[]},{"cell_type":"code","source":"test_data_for_h2o = _convert_h2oframe_to_numeric(h2o.H2OFrame(X_test), feature_cols)\nh20ai_best_model_pred = aml.leader.predict(test_data_for_h2o)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:16:13.284748Z","iopub.execute_input":"2021-12-31T16:16:13.285105Z","iopub.status.idle":"2021-12-31T16:18:37.496272Z","shell.execute_reply.started":"2021-12-31T16:16:13.285059Z","shell.execute_reply":"2021-12-31T16:18:37.495555Z"},"trusted":true},"execution_count":329,"outputs":[]},{"cell_type":"code","source":"submission_data = test_data[['Id']]\nsubmission_data[category] = list(h20ai_best_model_pred.as_data_frame()['p1'])\nsubmission_data.to_csv(category.replace(' ','_') + '_h20_ai_best_model.csv', index=False)\nprint(submission_data.shape)\nsubmission_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T16:18:37.499132Z","iopub.execute_input":"2021-12-31T16:18:37.499794Z","iopub.status.idle":"2021-12-31T16:18:37.551041Z","shell.execute_reply.started":"2021-12-31T16:18:37.499741Z","shell.execute_reply":"2021-12-31T16:18:37.550395Z"},"trusted":true},"execution_count":330,"outputs":[]},{"cell_type":"markdown","source":"#### Binary GAN-BERT","metadata":{"papermill":{"duration":0.028899,"end_time":"2021-12-27T11:31:49.921415","exception":false,"start_time":"2021-12-27T11:31:49.892516","status":"completed"},"tags":[],"id":"a99ecf3d"}},{"cell_type":"code","source":"if torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"papermill":{"duration":0.030123,"end_time":"2021-12-27T11:31:49.980126","exception":false,"start_time":"2021-12-27T11:31:49.950003","status":"completed"},"tags":[],"id":"7c009c06","outputId":"d72a91a8-c41e-4080-ffbc-37a776d3e8fa","execution":{"iopub.status.busy":"2021-12-31T15:58:26.005913Z","iopub.status.idle":"2021-12-31T15:58:26.006425Z","shell.execute_reply.started":"2021-12-31T15:58:26.006232Z","shell.execute_reply":"2021-12-31T15:58:26.006254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#--------------------------------\n#  Transformer parameters\n#--------------------------------\nmax_seq_length = 512\nbatch_size = 8\n\n#--------------------------------\n#  GAN-BERT specific parameters\n#--------------------------------\n# number of hidden layers in the generator, \n# each of the size of the output space\nnum_hidden_layers_g = 1; \n# number of hidden layers in the discriminator, \n# each of the size of the input space\nnum_hidden_layers_d = 1; \n# size of the generator's input noisy vectors\nnoise_size = 100\n# dropout to be applied to discriminator's input vectors\nout_dropout_rate = 0.3\n\n# Replicate labeled data to balance poorly represented datasets, \n# e.g., less than 1% of labeled material\napply_balance = True\n\n#--------------------------------\n#  Optimization parameters\n#--------------------------------\nlearning_rate_discriminator = 5e-5\nlearning_rate_generator = 5e-5\nepsilon = 1e-8\n# num_train_epochs = 5\nmulti_gpu = True\n# Scheduler\napply_scheduler = False\nwarmup_proportion = 0.1\n# Print\nprint_each_n_step = 5\n\n#--------------------------------\n#  Adopted Tranformer model\n#--------------------------------\n# Since this version is compatible with Huggingface transformers, you can uncomment\n# (or add) transformer models compatible with GAN\n\n# model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\nmodel_name = \"bert-base-uncased\"\n#model_name = \"roberta-base\"\n#model_name = \"albert-base-v2\"\n#model_name = \"xlm-roberta-base\"\n#model_name = \"amazon/bort\"\n\n#--------------------------------\n#  Retrieve the TREC QC Dataset\n#--------------------------------\n# ! git clone https://github.com/crux82/ganbert\n\n# #  NOTE: in this setting 50 classes are involved","metadata":{"papermill":{"duration":0.029294,"end_time":"2021-12-27T11:31:50.038049","exception":false,"start_time":"2021-12-27T11:31:50.008755","status":"completed"},"tags":[],"id":"3ba44779","execution":{"iopub.status.busy":"2021-12-31T15:58:26.008033Z","iopub.status.idle":"2021-12-31T15:58:26.008348Z","shell.execute_reply.started":"2021-12-31T15:58:26.008178Z","shell.execute_reply":"2021-12-31T15:58:26.008200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = AutoModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"papermill":{"duration":0.028376,"end_time":"2021-12-27T11:31:50.096363","exception":false,"start_time":"2021-12-27T11:31:50.067987","status":"completed"},"tags":[],"id":"7145880d","outputId":"31c5aa37-0c88-496f-e226-d607cc2924f7","execution":{"iopub.status.busy":"2021-12-31T15:58:26.009190Z","iopub.status.idle":"2021-12-31T15:58:26.009780Z","shell.execute_reply.started":"2021-12-31T15:58:26.009466Z","shell.execute_reply":"2021-12-31T15:58:26.009500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories","metadata":{"id":"yyrGoYAvXPmc","outputId":"3b751f28-3aae-4c55-8876-7bc87194fe95","execution":{"iopub.status.busy":"2021-12-31T15:58:26.011400Z","iopub.status.idle":"2021-12-31T15:58:26.011781Z","shell.execute_reply.started":"2021-12-31T15:58:26.011605Z","shell.execute_reply":"2021-12-31T15:58:26.011628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train_data.copy()","metadata":{"id":"pRibNqb5XStq","execution":{"iopub.status.busy":"2021-12-31T15:58:26.013046Z","iopub.status.idle":"2021-12-31T15:58:26.013379Z","shell.execute_reply.started":"2021-12-31T15:58:26.013207Z","shell.execute_reply":"2021-12-31T15:58:26.013228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test,train=train_test_split(data,test_size=0.8,stratify=data[category],random_state=SEED)\n\n# Filter for only protocol related queries\n# train = train[train['Protocol/ Non-protocol Query']=='Protocol Query'].reset_index(drop = True)\n# test = test[test['Protocol/ Non-protocol Query']=='Protocol Query'].reset_index(drop = True)\n\ntrain_list=[]\ntest_list=[]\nfor ind,row in train.iterrows(): \n    text=row['Review cleaned for transformers']\n    for i in [category]:\n        if row[i]==1:\n            train_list.append((text,'positive'))\n        else:\n            train_list.append((text,'negative'))\nfor ind,row in test.iterrows(): \n    text=row['Review cleaned for transformers']\n    for i in [category]:\n        if row[i]==1:\n            test_list.append((text,'positive'))\n        else:\n            test_list.append((text,'negative'))","metadata":{"papermill":{"duration":0.028136,"end_time":"2021-12-27T11:31:50.210476","exception":false,"start_time":"2021-12-27T11:31:50.18234","status":"completed"},"tags":[],"id":"454e626f","execution":{"iopub.status.busy":"2021-12-31T15:58:26.014555Z","iopub.status.idle":"2021-12-31T15:58:26.014927Z","shell.execute_reply.started":"2021-12-31T15:58:26.014761Z","shell.execute_reply":"2021-12-31T15:58:26.014783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_list),len(test_list)","metadata":{"papermill":{"duration":0.029409,"end_time":"2021-12-27T11:31:50.26953","exception":false,"start_time":"2021-12-27T11:31:50.240121","status":"completed"},"tags":[],"id":"44116ae7","outputId":"78d4ca96-dfe3-4d69-f7c6-9c901fabf546","execution":{"iopub.status.busy":"2021-12-31T15:58:26.015819Z","iopub.status.idle":"2021-12-31T15:58:26.016125Z","shell.execute_reply.started":"2021-12-31T15:58:26.015961Z","shell.execute_reply":"2021-12-31T15:58:26.015983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n    '''\n    Generate a Dataloader given the input examples, eventually masked if they are \n    to be considered NOT labeled.\n    '''\n    examples = []\n\n    # Count the percentage of labeled examples  \n    num_labeled_examples = 0\n    for label_mask in label_masks:\n        if label_mask: \n            num_labeled_examples += 1\n    label_mask_rate = num_labeled_examples/len(input_examples)\n\n    # if required it applies the balance\n    for index, ex in enumerate(input_examples): \n        if label_mask_rate == 1 or not balance_label_examples:\n            examples.append((ex, label_masks[index]))\n    else:\n      # IT SIMULATE A LABELED EXAMPLE\n      if label_masks[index]:\n        balance = int(1/label_mask_rate)\n        balance = int(math.log(balance,2))\n        if balance < 1:\n            balance = 1\n        for b in range(0, int(balance)):\n            examples.append((ex, label_masks[index]))\n        else:\n            examples.append((ex, label_masks[index]))\n\n    #-----------------------------------------------\n    # Generate input examples to the Transformer\n    #-----------------------------------------------\n    input_ids = []\n    input_mask_array = []\n    label_mask_array = []\n    label_id_array = []\n\n    # Tokenization \n    for (text, label_mask) in examples:\n        encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n        input_ids.append(encoded_sent)\n        label_id_array.append(label_map[text[1]])\n        label_mask_array.append(label_mask)\n\n    # Attention to token (to ignore padded input wordpieces)\n    for sent in input_ids:\n        att_mask = [int(token_id > 0) for token_id in sent]                          \n        input_mask_array.append(att_mask)\n    # Convertion to Tensor\n    input_ids = torch.tensor(input_ids) \n    input_mask_array = torch.tensor(input_mask_array)\n    label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n    label_mask_array = torch.tensor(label_mask_array)\n\n    # Building the TensorDataset\n    dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n\n    if do_shuffle:\n        sampler = RandomSampler\n    else:\n        sampler = SequentialSampler\n\n    # Building the DataLoader\n    return DataLoader(\n              dataset,  # The training samples.\n              sampler = sampler(dataset), \n              batch_size = batch_size) # Trains with this batch size.\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"papermill":{"duration":0.028554,"end_time":"2021-12-27T11:31:50.327967","exception":false,"start_time":"2021-12-27T11:31:50.299413","status":"completed"},"tags":[],"id":"2237bb9f","execution":{"iopub.status.busy":"2021-12-31T15:58:26.017515Z","iopub.status.idle":"2021-12-31T15:58:26.017873Z","shell.execute_reply.started":"2021-12-31T15:58:26.017695Z","shell.execute_reply":"2021-12-31T15:58:26.017717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_list=['negative','positive']","metadata":{"papermill":{"duration":0.028653,"end_time":"2021-12-27T11:31:50.386128","exception":false,"start_time":"2021-12-27T11:31:50.357475","status":"completed"},"tags":[],"id":"25ee9904","execution":{"iopub.status.busy":"2021-12-31T15:58:26.019144Z","iopub.status.idle":"2021-12-31T15:58:26.019449Z","shell.execute_reply.started":"2021-12-31T15:58:26.019289Z","shell.execute_reply":"2021-12-31T15:58:26.019311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_map = {}\nfor (i, label) in enumerate(label_list):\n    label_map[label] = i","metadata":{"papermill":{"duration":0.028352,"end_time":"2021-12-27T11:31:50.443757","exception":false,"start_time":"2021-12-27T11:31:50.415405","status":"completed"},"tags":[],"id":"b28d8850","execution":{"iopub.status.busy":"2021-12-31T15:58:26.020515Z","iopub.status.idle":"2021-12-31T15:58:26.021055Z","shell.execute_reply.started":"2021-12-31T15:58:26.020843Z","shell.execute_reply":"2021-12-31T15:58:26.020868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_map","metadata":{"papermill":{"duration":0.028439,"end_time":"2021-12-27T11:31:50.500897","exception":false,"start_time":"2021-12-27T11:31:50.472458","status":"completed"},"tags":[],"id":"6123d5e7","outputId":"439f3ee0-a5fa-4f13-d843-71a7e088cf8d","execution":{"iopub.status.busy":"2021-12-31T15:58:26.022354Z","iopub.status.idle":"2021-12-31T15:58:26.022709Z","shell.execute_reply.started":"2021-12-31T15:58:26.022510Z","shell.execute_reply":"2021-12-31T15:58:26.022532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_label_masks = np.ones(len(train_list), dtype=bool)\ntrain_dataloader = generate_data_loader(train_list, train_label_masks, label_map, do_shuffle = True, balance_label_examples = False)\n\ntest_label_masks = np.ones(len(test_list), dtype=bool)\ntest_dataloader = generate_data_loader(test_list, test_label_masks, label_map, do_shuffle = True, balance_label_examples = False)","metadata":{"papermill":{"duration":0.029237,"end_time":"2021-12-27T11:31:50.561744","exception":false,"start_time":"2021-12-27T11:31:50.532507","status":"completed"},"tags":[],"id":"abb3bf8e","execution":{"iopub.status.busy":"2021-12-31T15:58:26.025133Z","iopub.status.idle":"2021-12-31T15:58:26.025579Z","shell.execute_reply.started":"2021-12-31T15:58:26.025381Z","shell.execute_reply":"2021-12-31T15:58:26.025402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------\n#   The Generator as in \n#   https://www.aclweb.org/anthology/2020.acl-main.191/\n#   https://github.com/crux82/ganbert\n#------------------------------\nclass Generator(nn.Module):\n    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n        super(Generator, self).__init__()\n        layers = []\n        hidden_sizes = [noise_size] + hidden_sizes\n        for i in range(len(hidden_sizes)-1):\n            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n\n        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, noise):\n        output_rep = self.layers(noise)\n        return output_rep\n\n#------------------------------\n#   The Discriminator\n#   https://www.aclweb.org/anthology/2020.acl-main.191/\n#   https://github.com/crux82/ganbert\n#------------------------------\nclass Discriminator(nn.Module):\n    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n        super(Discriminator, self).__init__()\n        self.input_dropout = nn.Dropout(p=dropout_rate)\n        layers = []\n        hidden_sizes = [input_size] + hidden_sizes\n        for i in range(len(hidden_sizes)-1):\n            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n\n        self.layers = nn.Sequential(*layers) #per il flatten\n        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, input_rep):\n        input_rep = self.input_dropout(input_rep)\n        last_rep = self.layers(input_rep)\n        logits = self.logit(last_rep)\n        probs = self.softmax(logits)\n        return last_rep, logits, probs","metadata":{"papermill":{"duration":0.028076,"end_time":"2021-12-27T11:31:50.61824","exception":false,"start_time":"2021-12-27T11:31:50.590164","status":"completed"},"tags":[],"id":"b5cd6f9c","execution":{"iopub.status.busy":"2021-12-31T15:58:26.027033Z","iopub.status.idle":"2021-12-31T15:58:26.027371Z","shell.execute_reply.started":"2021-12-31T15:58:26.027202Z","shell.execute_reply":"2021-12-31T15:58:26.027224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"id":"XYh3alyhYoDi","execution":{"iopub.status.busy":"2021-12-31T15:58:26.028926Z","iopub.status.idle":"2021-12-31T15:58:26.029249Z","shell.execute_reply.started":"2021-12-31T15:58:26.029078Z","shell.execute_reply":"2021-12-31T15:58:26.029099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The config file is required to get the dimension of the vector produced by \n# the underlying transformer\nconfig = AutoConfig.from_pretrained(model_name)\nhidden_size = int(config.hidden_size)\n# Define the number and width of hidden layers\nhidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\nhidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n\n#-------------------------------------------------\n#   Instantiate the Generator and Discriminator\n#-------------------------------------------------\ngenerator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\ndiscriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n\n# Put everything in the GPU if available\nif torch.cuda.is_available():    \n    generator.cuda()\n    discriminator.cuda()\n    transformer.cuda()\n    if multi_gpu:\n        transformer = torch.nn.DataParallel(transformer)\n\n# print(config)","metadata":{"id":"B0fPiFQZYoFy","execution":{"iopub.status.busy":"2021-12-31T15:58:26.030463Z","iopub.status.idle":"2021-12-31T15:58:26.030990Z","shell.execute_reply.started":"2021-12-31T15:58:26.030793Z","shell.execute_reply":"2021-12-31T15:58:26.030816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train_epochs = 3\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n#models parameters\ntransformer_vars = [i for i in transformer.parameters()]\nd_vars = transformer_vars + [v for v in discriminator.parameters()]\ng_vars = [v for v in generator.parameters()]\n\n#optimizer\ndis_optimizer = torch.optim.Adam(d_vars, lr=learning_rate_discriminator)\ngen_optimizer = torch.optim.Adam(g_vars, lr=learning_rate_generator) \n\n#scheduler\nif apply_scheduler:\n    num_train_examples = len(train_examples)\n    num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n    num_warmup_steps = int(num_train_steps * warmup_proportion)\n\n    scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n                                           num_warmup_steps = num_warmup_steps)\n    scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n                                           num_warmup_steps = num_warmup_steps)\n\n# For each epoch...\nfor epoch_i in range(0, num_train_epochs):\n    # ========================================\n    #               Training\n    # ========================================\n    # Perform one full pass over the training set.\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    tr_g_loss = 0\n    tr_d_loss = 0\n\n    # Put the model into training mode.\n    transformer.train() \n    generator.train()\n    discriminator.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every print_each_n_step batches.\n        if step % print_each_n_step == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        b_label_mask = batch[3].to(device)\n     \n        # Encode real data in the Transformer\n        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n        hidden_states = model_outputs[-1]\n        \n        # Generate fake data that should have the same distribution of the ones\n        # encoded by the transformer. \n        # First noisy input are used in input to the Generator\n        noise = torch.zeros(b_input_ids.shape[0],noise_size, device=device).uniform_(0, 1)\n        # Gnerate Fake data\n        gen_rep = generator(noise)\n\n        # Generate the output of the Discriminator for real and fake data.\n        # First, we put together the output of the tranformer and the generator\n        disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n        # Then, we select the output of the disciminator\n        features, logits, probs = discriminator(disciminator_input)\n\n        # Finally, we separate the discriminator's output for the real and fake\n        # data\n        features_list = torch.split(features, len(hidden_states))\n        D_real_features = features_list[0]\n        D_fake_features = features_list[1]\n      \n        logits_list = torch.split(logits, len(hidden_states))\n        D_real_logits = logits_list[0]\n        D_fake_logits = logits_list[1]\n        \n        probs_list = torch.split(probs, len(hidden_states))\n        D_real_probs = probs_list[0]\n        D_fake_probs = probs_list[1]\n\n        #---------------------------------\n        #  LOSS evaluation\n        #---------------------------------\n        # Generator's LOSS estimation\n        g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n        g_loss = g_loss_d + g_feat_reg\n  \n        # Disciminator's LOSS estimation\n        logits = D_real_logits[:,0:-1]\n        log_probs = F.log_softmax(logits, dim=-1)\n        # The discriminator provides an output for labeled and unlabeled real data\n        # so the loss evaluated for unlabeled data is ignored (masked)\n        label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n        per_example_loss = -torch.sum(label2one_hot.float() * log_probs, dim=-1)\n#         per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n        labeled_example_count = per_example_loss.type(torch.float32).numel()\n\n        # It may be the case that a batch does not contain labeled examples, \n        # so the \"supervised loss\" in this case is not evaluated\n        if labeled_example_count == 0:\n            D_L_Supervised = 0\n        else:\n            D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n                 \n        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n        D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n\n        #---------------------------------\n        #  OPTIMIZATION\n        #---------------------------------\n        # Avoid gradient accumulation\n        gen_optimizer.zero_grad()\n        dis_optimizer.zero_grad()\n\n        # Calculate weigth updates\n        # retain_graph=True is required since the underlying graph will be deleted after backward\n        g_loss.backward(retain_graph=True)\n        d_loss.backward() \n        \n        # Apply modifications\n        gen_optimizer.step()\n        dis_optimizer.step()\n\n        # A detail log of the individual losses\n        #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n        #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n        #             g_loss_d, g_feat_reg))\n\n        # Save the losses to print them later\n        tr_g_loss += g_loss.item()\n        tr_d_loss += d_loss.item()\n\n        # Update the learning rate with the scheduler\n        if apply_scheduler:\n            scheduler_d.step()\n            scheduler_g.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n    avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n    print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n    print(\"  Training epoch took: {:}\".format(training_time))\n        \n    # ========================================\n    #     TEST ON THE EVALUATION DATASET\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our test set.\n    print(\"\")\n    print(\"Running Test...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    transformer.eval() #maybe redundant\n    discriminator.eval()\n    generator.eval()\n\n    # Tracking variables \n    total_test_accuracy = 0\n   \n    total_test_loss = 0\n    nb_test_steps = 0\n\n    all_preds = []\n    all_labels_ids = []\n\n    #loss\n    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n\n    # Evaluate data for one epoch\n    for batch in test_dataloader:\n        \n        # Unpack this training batch from our dataloader. \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n            hidden_states = model_outputs[-1]\n            _, logits, probs = discriminator(hidden_states)\n            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n            filtered_logits = logits[:,0:-1]\n            # Accumulate the test loss.\n            total_test_loss += nll_loss(filtered_logits, b_labels)\n            \n        # Accumulate the predictions and the input labels\n        _, preds = torch.max(filtered_logits, 1)\n        all_preds += preds.detach().tolist()\n        all_labels_ids += b_labels.detach().tolist()\n\n    # Report the final accuracy for this validation run.\n    all_preds = np.array(all_preds)\n    all_labels_ids = np.array(all_labels_ids)\n    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n    print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_test_loss = total_test_loss / len(test_dataloader)\n    avg_test_loss = avg_test_loss.item()\n    \n    # Measure how long the validation run took.\n    test_time = format_time(time.time() - t0)\n    \n    print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n    print(\"  Test took: {:}\".format(test_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss generator': avg_train_loss_g,\n            'Training Loss discriminator': avg_train_loss_d,\n            'Valid. Loss': avg_test_loss,\n            'Valid. Accur.': test_accuracy,\n            'Training Time': training_time,\n            'Test Time': test_time\n        }\n    )","metadata":{"id":"N4X4PSELYoIa","outputId":"62848f4c-6e7c-4b77-9cda-a170e6648e29","execution":{"iopub.status.busy":"2021-12-31T15:58:26.032089Z","iopub.status.idle":"2021-12-31T15:58:26.032371Z","shell.execute_reply.started":"2021-12-31T15:58:26.032224Z","shell.execute_reply":"2021-12-31T15:58:26.032239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # To load previously fine-tuned trained model\n# model_upd_nm = model_name.replace('-','_')\n# c_tokenizer = AutoTokenizer.from_pretrained(f\"./Model Data/{model_upd_nm}/\")\n# transformer = AutoModel.from_pretrained(model_name)\n# transformer.load_state_dict(torch.load(f'./Model Data/{model_upd_nm}/'))\n\n# generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n# discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n\n# generator.load_state_dict(torch.load(f'Model Data/GAN_BERT_binary_generator_{category}.pth')['model_state_dict'])\n# discriminator.load_state_dict(torch.load(f'Model Data/GAN_BERT_binary_discriminator_{category}.pth')['model_state_dict'])\n\n# # Put everything in the GPU if available\n# if torch.cuda.is_available():    \n#     generator.cuda()\n#     discriminator.cuda()\n#     transformer.cuda()\n#     if multi_gpu:\n#         transformer = torch.nn.DataParallel(transformer)","metadata":{"id":"IHTytloVYoK4","execution":{"iopub.status.busy":"2021-12-31T15:58:26.033423Z","iopub.status.idle":"2021-12-31T15:58:26.033751Z","shell.execute_reply.started":"2021-12-31T15:58:26.033561Z","shell.execute_reply":"2021-12-31T15:58:26.033595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\")\nprint(\"Running Test...\")\n\nt0 = time.time()\n\n# Put the model in evaluation mode--the dropout layers behave differently\n# during evaluation.\ntransformer.eval() #maybe redundant\ndiscriminator.eval()\ngenerator.eval()\n\n# Tracking variables \ntotal_test_accuracy = 0\n\ntotal_test_loss = 0\nnb_test_steps = 0\n\nall_preds = []\nall_labels_ids = []\npred_prob=[]\n\n#loss\nnll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\nfor batch in test_dataloader:\n\n    # Unpack this training batch from our dataloader. \n    b_input_ids = batch[0].to(device)\n    b_input_mask = batch[1].to(device)\n    b_labels = batch[2].to(device)\n\n    # Tell pytorch not to bother with constructing the compute graph during\n    # the forward pass, since this is only needed for backprop (training).\n    with torch.no_grad():        \n        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n        hidden_states = model_outputs[-1]\n        _, logits, probs = discriminator(hidden_states)\n        ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n        filtered_logits = logits[:,0:-1]\n        # Accumulate the test loss.\n        total_test_loss += nll_loss(filtered_logits, b_labels)\n\n    # Accumulate the predictions and the input labels\n    pred_prob += probs[:,0:-1].detach().tolist()\n    _, preds = torch.max(filtered_logits, 1)\n    all_preds += preds.detach().tolist()\n    all_labels_ids += b_labels.detach().tolist()\n    \n# Report the final accuracy for this validation run.\nall_preds = np.array(all_preds)\nall_labels_ids = np.array(all_labels_ids)\ntest_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\nprint(\"  Accuracy: {0:.3f}\".format(test_accuracy))","metadata":{"id":"01pS94CDYoNb","execution":{"iopub.status.busy":"2021-12-31T15:58:26.034793Z","iopub.status.idle":"2021-12-31T15:58:26.035088Z","shell.execute_reply.started":"2021-12-31T15:58:26.034930Z","shell.execute_reply":"2021-12-31T15:58:26.034951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=pd.DataFrame(all_preds,columns=['Predicted'])\ny_true=pd.DataFrame(all_labels_ids,columns=['Actual'])\nfor i in range(len(label_list)):\n    y_pred[label_list[i]]=[1 if x==i else 0 for x in all_preds]\n    y_true[label_list[i]]=[1 if x==i else 0 for x in all_labels_ids]\n    \ny_pred=y_pred['positive']\ny_true=y_true['positive']","metadata":{"id":"1jByApYZYoQw","execution":{"iopub.status.busy":"2021-12-31T15:58:26.036069Z","iopub.status.idle":"2021-12-31T15:58:26.036379Z","shell.execute_reply.started":"2021-12-31T15:58:26.036220Z","shell.execute_reply":"2021-12-31T15:58:26.036241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true.sum()","metadata":{"id":"mP76nhTbZtAl","execution":{"iopub.status.busy":"2021-12-31T15:58:26.037817Z","iopub.status.idle":"2021-12-31T15:58:26.038137Z","shell.execute_reply.started":"2021-12-31T15:58:26.037976Z","shell.execute_reply":"2021-12-31T15:58:26.037998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_reports=[]\n# train_reports=[]\n\nfor i in range(len(label_list)):\n    try:\n        report_test,t=classification_metrics(y_true.iloc[:,i].tolist(), y_pred.iloc[:,i].tolist())\n        test_reports.append(pd.DataFrame(report_test,[label_list[i]]))\n    except:\n        print(label_list[i])\n        \ntry:\n    test_df=pd.concat(test_reports)\nexcept:\n    test_df = pd.DataFrame(report_test,[label_list[i]])\n# train_df=pd.concat(train_reports)","metadata":{"id":"sBwbc-1gZtDE","execution":{"iopub.status.busy":"2021-12-31T15:58:26.039605Z","iopub.status.idle":"2021-12-31T15:58:26.039932Z","shell.execute_reply.started":"2021-12-31T15:58:26.039768Z","shell.execute_reply":"2021-12-31T15:58:26.039790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\")\nprint(\"Running Train...\")\n\nt0 = time.time()\n\n# Put the model in evaluation mode--the dropout layers behave differently\n# during evaluation.\ntransformer.eval() #maybe redundant\ndiscriminator.eval()\ngenerator.eval()\n\n# Tracking variables \ntotal_test_accuracy = 0\n\ntotal_test_loss = 0\nnb_test_steps = 0\n\nall_preds = []\nall_labels_ids = []\npred_prob=[]\n\n#loss\nnll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\nfor batch in train_dataloader:\n\n    # Unpack this training batch from our dataloader. \n    b_input_ids = batch[0].to(device)\n    b_input_mask = batch[1].to(device)\n    b_labels = batch[2].to(device)\n\n    # Tell pytorch not to bother with constructing the compute graph during\n    # the forward pass, since this is only needed for backprop (training).\n    with torch.no_grad():        \n        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n        hidden_states = model_outputs[-1]\n        _, logits, probs = discriminator(hidden_states)\n        ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n        filtered_logits = logits[:,0:-1]\n        # Accumulate the test loss.\n        total_test_loss += nll_loss(filtered_logits, b_labels)\n\n    # Accumulate the predictions and the input labels\n    pred_prob += probs[:,0:-1].detach().tolist()\n    _, preds = torch.max(filtered_logits, 1)\n    all_preds += preds.detach().tolist()\n    all_labels_ids += b_labels.detach().tolist()\n    \n# Report the final accuracy for this validation run.\nall_preds = np.array(all_preds)\nall_labels_ids = np.array(all_labels_ids)\ntest_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\nprint(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n\ny_pred=pd.DataFrame(all_preds,columns=['Predicted'])\ny_true=pd.DataFrame(all_labels_ids,columns=['Actual'])\nfor i in range(len(label_list)):\n    y_pred[label_list[i]]=[1 if x==i else 0 for x in all_preds]\n    y_true[label_list[i]]=[1 if x==i else 0 for x in all_labels_ids]\n    \ny_pred=y_pred['positive']\ny_true=y_true['positive']\n\ntrain_reports=[]\n\nfor i in range(len(label_list)):\n    try:\n        report_test,t=classification_metrics(y_true.iloc[:,i].tolist(), y_pred.iloc[:,i].tolist())\n        train_reports.append(pd.DataFrame(report_test,[label_list[i]]))\n    except:\n        print(label_list[i])\n        \ntry:\n    train_df=pd.concat(train_reports)\nexcept:\n    train_df = pd.DataFrame(report_test,[label_list[i]])","metadata":{"id":"R494ZbWrZtFN","execution":{"iopub.status.busy":"2021-12-31T15:58:26.041650Z","iopub.status.idle":"2021-12-31T15:58:26.041989Z","shell.execute_reply.started":"2021-12-31T15:58:26.041824Z","shell.execute_reply":"2021-12-31T15:58:26.041846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame([x[1] for x in pred_prob]).rename(columns = {0:'Predicted probability'}).describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:58:26.042851Z","iopub.status.idle":"2021-12-31T15:58:26.043151Z","shell.execute_reply.started":"2021-12-31T15:58:26.042991Z","shell.execute_reply":"2021-12-31T15:58:26.043013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"id":"TuyaYvy_ZtG-","execution":{"iopub.status.busy":"2021-12-31T15:58:26.044342Z","iopub.status.idle":"2021-12-31T15:58:26.044687Z","shell.execute_reply.started":"2021-12-31T15:58:26.044481Z","shell.execute_reply":"2021-12-31T15:58:26.044502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"id":"F9_DyVMSZtKY","execution":{"iopub.status.busy":"2021-12-31T15:58:26.045577Z","iopub.status.idle":"2021-12-31T15:58:26.045922Z","shell.execute_reply.started":"2021-12-31T15:58:26.045758Z","shell.execute_reply":"2021-12-31T15:58:26.045780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Save model\n\n# #Save config\n# model_upd_nm = model_name.replace('-','_')\n# temp = AutoModel.from_pretrained(model_name)\n# temp.save_pretrained(f'./Model Data/{model_upd_nm}/')\n\n# torch.save(transformer.module.state_dict(), f\"./Model Data/{model_upd_nm}.pth\")\n# tokenizer.save_pretrained(f\"./Model Data/{model_name.replace('-','_')}/\")\n\n# torch.save(\n#     {\n#         \"model_state_dict\": generator.state_dict()\n#     },\n#     f'Model Data/GAN_BERT_binary_generator_{category}.pth',\n# )\n# torch.save(\n#     {\n#         \"model_state_dict\": discriminator.state_dict()\n#     },\n#     f'Model Data/GAN_BERT_binary_discriminator_{category}.pth',\n# )","metadata":{"papermill":{"duration":0.029275,"end_time":"2021-12-27T11:31:50.677515","exception":false,"start_time":"2021-12-27T11:31:50.64824","status":"completed"},"tags":[],"id":"53fff942","execution":{"iopub.status.busy":"2021-12-31T15:58:26.046813Z","iopub.status.idle":"2021-12-31T15:58:26.047117Z","shell.execute_reply.started":"2021-12-31T15:58:26.046957Z","shell.execute_reply":"2021-12-31T15:58:26.046977Z"},"trusted":true},"execution_count":null,"outputs":[]}]}